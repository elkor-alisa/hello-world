# Основные принципы Unicode. Выбираем UTF

## Зачем нужно кодирование

Компьютерная программа — это некоторый набор информации, который хранится в байтах, единицах обработки и хранения информации. 1 байт состоит из 8 бит. 

Люди не обладают способностью читать байты. Мы можем понять код только если его перевести на понятный нам язык, то есть превратить некоторый набор байтов в набор символов.

Процесс перевода байтов в символы называется **декодированием**, а обратный процесс — наоборот, **кодированием**. 

Компьютерные программы обычно пишутся не для одиночного использования, поэтому ими пользуются разные люди разных национальностей, которые говорят на разных языках. Из этого следует, что нам очень важно знать, как именно закодированы строки нашей программы, иначе мы не сможем правильно их интерпретировать или показать пользователю.

Поэтому нам нужны стандарты кодирования, чтобы сопоставлять символы алфавита байтам.

## Первые попытки стандартизации

Одним из первых известных стандартов стал **ASCII** и создан он был для символов латинского алфавита. Это таблица из 128 знаков, в которой каждому символу соответствует некоторый двоичный код. Для латинского алфавита использовались числа от 32 до 127, символы до 32 были служебными.

Работало это так: программное обеспечение выделяло в памяти 1 байт или 8 бит информации для соответствия одному символу. Однако коды таблицы использовали для хранения информации 7 бит, потому что 128 символов — это 2⁷. Оставшийся бит использовали для контроля: по нему было легко понять, какой байт перед нами — одиночный или часть последовательности.

Большая часть компьютеров использовали регистры в 8 бит, поэтому с таким стандартом было не только легко хранить в памяти любой символ ASCII, но даже оставался лишний бит экономии. 

Если бы латинский алфавит был единственным в мире, всё бы на этом и закончилось. Но люди говорят на разных языках и используют разные алфавиты, поэтому стали появляться другие стандарты кодирования, они не совпадали с ASCII, и если требовалось перенести строки программы на другой компьютер, начиналась масса проблем с декодированием.

В конечном итоге было принято решение создать стандарт, который объединит все возможные стандарты кодирования в один, чтобы ни у кого никогда не возникало подобных проблем. Так появился **Unicode** — стандарт кодирования с амбициозной целью покрыть все возможные символы в мире.

## Один стандарт, чтобы править всеми

Стандарт представляет собой огромный набор графических символов **Universal Character Set** (UCS), в котором предусмотрены позиции от 0 до 1 114 111. Символы организованы в соответствии с частотой их употребления по **плоскостям**. Всего есть 17 плоскостей, каждая из которых может содержать по 65 536 символов. Первая плоскость называется базовой, в неё входят все наиболее употребительные символы мира. Несколько плоскостей считаются дополнительными и в основном используются для исторических и редко используемых иероглифов, а также символов особого назначения. Есть и пустые пока не заполненные плоскости, и плоскости для частного использования.

В ASCII символ отображался на набор битов, которые можно хранить в памяти. В Unicode же символ отображается на абстрактную **кодовую точку** (code points) формата U + число в шестнадцатеричном исчислении. В этом основное отличие ASCII от Unicode. 

Unicode регулярно обновляется и пополняется символами по мере их появления. В данный момент в версии 12.1 используется 137 994 кодовых позиций.

Поскольку кодовая точка не задаёт байты, это всего лишь позиция некоторого символа в большой таблице Unicode, нам требуется **Unicode Transformation Format** (UTF), чтобы закодировать последовательность кодовых точек в байты по его правилам.

Форматов UTF несколько, выбор конкретного зависит от того, каким образом вы хотите организовать представление символов в памяти. 

Все кодировки могут кодировать один и тот же набор символов Unicode, но каждый символ в разных кодировках будет представлять разные последовательности байтов.

## Форматы кодирования Unicode

Рассмотрим две наиболее известные кодировки для Unicode и сделаем вывод, в чём их плюсы, минусы и основные различия.

### UTF-16

Когда Unicode только появился, использовалась кодировка с фиксированной шириной машинного слова, она называлась _UCS-2_ и подразумевала, что каждый символ равен 2 байтам. Используя UCS-2, можно было отобразить 65 536 кодовых точек, т. е. 2¹⁶.

Однако позже в Unicode включили дополнительные исторические символы, для которых нужно было больше code points, чем могла обеспечить UCS-2. К этому времени появилось уже много программных продуктов, чьи библиотеки были основаны на 16-разрядных регистрах строк, такие как Java и Windows NT. Поэтому чтобы такие платформы могли использовать дополнительные символы не меняя подход UCS-2, была введена кодировка **UTF-16**. 

В UTF-16 каждая кодовая точка кодируется либо одним, либо несколькими 16-битными словами. Таким образом, UTF-16 — это система кодирования с переменной длиной машинного слова. Если символ, который требуется закодировать, относится к плоскости не базовых, а дополнительных символов, UTF-16 использует так называемые **суррогатные пары** — комбинации из двух байтов. Комбинации собираются из числового диапазона базовой плоскости, но при этом гарантируются стандартом Unicode как недействительные в качестве базовых. Поэтому кодовая точка в UTF-16 может быть представлена либо в 16, либо в 32 битах в зависимости от типа символа.

Поскольку символ в UTF-16 — это одна или две пары байтов, здесь также важно понимать, в какой последовательности они идут друг за другом, какой из байтов должен быть первым. От этого зависит правильное декодирование символа. 

Чтобы это определить, используется **метка порядка байтов**. Это метка U+FEFF — в Unicode она не кодирует никакой символ, а используется специально для этих целей. 

Если при считывании строки получилось U+FEFF, значит, порядок байтов прямой и первым идёт старший байт, а кодировка называется UTF-16BE (big endian). Если считалось U+FFFE, значит, порядок обратный — первым идёт младший байт, а кодировка называется UTF-16LE (little endian).

#### Плюсы

Если предстоит работать в основном не с латинским алфавитом, эта кодировка выгоднее, поскольку чаще использует 2 байта на символ, тогда как в UTF-8 на нестандартный символ потребуется от 3 байтов.

#### Минусы

Это кодировка с переменной длиной символа, поэтому не получится точно рассчитать занимаемый размер строки в памяти.

Необходимо знать байтовый порядок для правильного декодирования.

### UTF-8

Как мы помним, формат UTF-16 использует 16 бит на один символ. Если использовать латинский алфавит, чьи символы находятся в плоскости самых употребительных, окажется, что при таком кодировании в одном слове получается очень много нулей, которые нельзя убрать, поскольку они означают code points для каждого символа, но при этом эти нули занимают лишнее место в памяти.

Поэтому появилась кодировка **UTF-8**, совместимая с ASCII, в которой для каждой кодовой точки с номерами от 0 до 127 требовался только 1 байт. Больше байтов для хранения требовалось только для позиций кодовых точек начиная со 128 и выше.

Такой формат кодирования идеально подходил тем программным и сетевым протоколам, которые использовали 8-битный регистр строк. Благодаря UTF-8 у них появилась возможность поддерживать Unicode и при этом не занимать память широкими символами в 16 бит.

Символ в UTF-8 занимает от 1 до 4 байтов в зависимости от его кодовой плоскости. Для символов латинского алфавита потребуется только 1 байт, для европейских — 2 байта, для азиатских символов — минимум 2 байта и максимум 4 байта.

#### Плюсы

Для работы с символами из ASCII использование UTF-8 выгоднее с точки зрения памяти. 

Не нужно знать порядок байтов.

#### Минусы

Во всех остальных случаях при UTF-8 может потребоваться больше памяти, чем при UTF-16. Если вы работаете с большими объёмами текста, это может повлиять на производительность.

Это тоже кодировка с переменной длиной символа, что затрудняет манипуляции со строкой.

### ИТОГО

#### UTF-8

- Не требует знания порядка байтов
- Использует от 1 до 4 байтов на символ
- Совместим с ASCII

#### UTF-16

- Требует анализа байтового порядка
- Использует 2 или 4 байта на символ
- Не совместим с ASCII

## Что выбрать?

Очень часто выбор UTF определяет рабочая среда, в которой вы работаете.

Например, в Windows внутренне используется UTF-16, а в HTML5 рекомендуется UTF-8. 

Скорее всего, если вы не планируете использовать сложные азиатские, исторические или музыкальные символы и не ограничены требованиями языка или платформы, то и нет необходимости в UTF-16. 

Очевидно, что UTF-8 более распространена. UTF-16 более предсказуема, так как при отсутствии сложных символов можно предположить, что длина каждого символа окажется 16 бит. Но при сравнении она почти всегда будет использовать больше памяти, чем UTF-8 и также не избавит вас гарантированно от проблем с переменной длиной символа.
